#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¯äº¤æ‰€æ–°ä¸Šå¸‚ä¿¡æ¯çˆ¬è™«
çˆ¬å– https://www2.hkexnews.hk çš„IPOä¿¡æ¯
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime


def crawl_hk_ipo():
    """çˆ¬å–æ¸¯äº¤æ‰€ä¸»æ¿æ–°ä¸Šå¸‚ä¿¡æ¯"""
    
    url = "https://www2.hkexnews.hk/New-Listings/New-Listing-Information/Main-Board?sc_lang=zh-HK"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
    except requests.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return []
    
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # æŸ¥æ‰¾è¡¨æ ¼
    table = soup.find('table')
    if not table:
        print("æœªæ‰¾åˆ°è¡¨æ ¼")
        return []
    
    ipo_list = []
    
    # è·å–æ‰€æœ‰æ•°æ®è¡Œï¼ˆè·³è¿‡è¡¨å¤´ï¼‰
    rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´è¡Œ
    
    for row in rows:
        cols = row.find_all('td')
        if len(cols) < 5:
            continue
        
        # æå–è‚¡ä»½ä»£å·
        stock_code = cols[0].get_text(strip=True)
        
        # æå–è‚¡ä»½åç§°
        stock_name = cols[1].get_text(strip=True)
        
        # æå–æ–°ä¸Šå¸‚å…¬å‘Šé“¾æ¥
        listing_notice_link = ""
        listing_notice_a = cols[2].find('a')
        if listing_notice_a and listing_notice_a.get('href'):
            listing_notice_link = listing_notice_a.get('href')
        
        # æå–æ‹›è‚¡ç« ç¨‹é“¾æ¥
        prospectus_link = ""
        prospectus_a = cols[3].find('a')
        if prospectus_a and prospectus_a.get('href'):
            prospectus_link = prospectus_a.get('href')
        
        # æå–è‚¡ä»½é…å‘ç»“æœé“¾æ¥
        allotment_link = ""
        allotment_a = cols[4].find('a')
        if allotment_a and allotment_a.get('href'):
            allotment_link = allotment_a.get('href')
        
        ipo_info = {
            "è‚¡ä»½ä»£å·": stock_code,
            "è‚¡ä»½åç§°": stock_name,
            "æ–°ä¸Šå¸‚å…¬å‘Š": listing_notice_link,
            "æ‹›è‚¡ç« ç¨‹": prospectus_link,
            "è‚¡ä»½é…å‘ç»“æœ": allotment_link
        }
        
        ipo_list.append(ipo_info)
    
    return ipo_list


def save_results(ipo_list, filename="ipo_results.json"):
    """ä¿å­˜çˆ¬å–ç»“æœåˆ°JSONæ–‡ä»¶"""
    
    result = {
        "çˆ¬å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "æ•°æ®æ¥æº": "https://www2.hkexnews.hk/New-Listings/New-Listing-Information/Main-Board?sc_lang=zh-HK",
        "IPOåˆ—è¡¨": ipo_list
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"ç»“æœå·²ä¿å­˜åˆ° {filename}")


def print_results(ipo_list):
    """æ‰“å°çˆ¬å–ç»“æœ"""
    
    print("\n" + "=" * 80)
    print("æ¸¯äº¤æ‰€ä¸»æ¿æ–°ä¸Šå¸‚ä¿¡æ¯")
    print("=" * 80)
    
    for i, ipo in enumerate(ipo_list, 1):
        print(f"\nã€{i}ã€‘{ipo['è‚¡ä»½åç§°']} ({ipo['è‚¡ä»½ä»£å·']})")
        print("-" * 40)
        
        if ipo['æ–°ä¸Šå¸‚å…¬å‘Š']:
            print(f"  ğŸ“„ æ–°ä¸Šå¸‚å…¬å‘Š: {ipo['æ–°ä¸Šå¸‚å…¬å‘Š']}")
        
        if ipo['æ‹›è‚¡ç« ç¨‹']:
            print(f"  ğŸ“‹ æ‹›è‚¡ç« ç¨‹: {ipo['æ‹›è‚¡ç« ç¨‹']}")
        
        if ipo['è‚¡ä»½é…å‘ç»“æœ']:
            print(f"  ğŸ“Š è‚¡ä»½é…å‘ç»“æœ: {ipo['è‚¡ä»½é…å‘ç»“æœ']}")
    
    print("\n" + "=" * 80)
    print(f"å…±æ‰¾åˆ° {len(ipo_list)} æ¡IPOä¿¡æ¯")
    print("=" * 80)


def main():
    print("å¼€å§‹çˆ¬å–æ¸¯äº¤æ‰€æ–°ä¸Šå¸‚ä¿¡æ¯...")
    
    ipo_list = crawl_hk_ipo()
    
    if ipo_list:
        print_results(ipo_list)
        save_results(ipo_list)
    else:
        print("æœªè·å–åˆ°IPOä¿¡æ¯")


if __name__ == "__main__":
    main()





